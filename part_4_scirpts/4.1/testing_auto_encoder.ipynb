{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d32a1f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 | Train Loss: 19674.9568 | Val Loss: 17796.1710\n",
      "‚úÖ Saved new best model (val loss 17796.1710)\n",
      "Epoch 2/10 | Train Loss: 17513.4383 | Val Loss: 17485.0215\n",
      "‚úÖ Saved new best model (val loss 17485.0215)\n",
      "Epoch 3/10 | Train Loss: 19312.5051 | Val Loss: 18487.5304\n",
      "Epoch 4/10 | Train Loss: 2024285.7002 | Val Loss: 18373.2919\n",
      "Epoch 5/10 | Train Loss: 18622.7894 | Val Loss: 18250.3958\n",
      "‚èπ Early stopping triggered.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from auto_encoder_classes import VAE\n",
    "from auto_encoder_functions import vae_loss\n",
    "\n",
    "# --------------------------\n",
    "# Minimal Dataset Definition\n",
    "# --------------------------\n",
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "class MRISliceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Simple dataset for MRI slices (no masks).\"\"\"\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*.png\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)  # shape (1,H,W)\n",
    "        return img\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "TRAIN_DIR = \"../4.2/keras_png_slices_train\"\n",
    "VAL_DIR = \"../4.2/keras_png_slices_validate\"\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIM = 32\n",
    "EPOCHS = 10\n",
    "LR = 1e-3\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = MRISliceDataset(TRAIN_DIR)\n",
    "val_dataset = MRISliceDataset(VAL_DIR)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "vae = VAE(latent_dim=LATENT_DIM, input_shape=(1, 256, 256)).to(device)\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=LR)\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "patience, max_patience = 0, 3  # early stopping\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    # --------------------------\n",
    "    # Training\n",
    "    # --------------------------\n",
    "    vae.train()\n",
    "    total_train_loss = 0\n",
    "    for imgs in train_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        recon, mu, logvar = vae(imgs)\n",
    "        loss = vae_loss(recon, imgs, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # --------------------------\n",
    "    # Validation\n",
    "    # --------------------------\n",
    "    vae.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs in val_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            recon, mu, logvar = vae(imgs)\n",
    "            total_val_loss += vae_loss(recon, imgs, mu, logvar).item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    # --------------------------\n",
    "    # Checkpoint Saving\n",
    "    # --------------------------\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(vae.state_dict(), f\"vae_epoch{epoch+1}_loss{best_val_loss:.4f}.pth\")\n",
    "        print(f\"‚úÖ Saved new best model (val loss {best_val_loss:.4f})\")\n",
    "        patience = 0\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience >= max_patience:\n",
    "            print(\"‚èπ Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094d263",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'glob'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     38\u001b[39m LATENT_DIM = \u001b[32m32\u001b[39m  \u001b[38;5;66;03m# must match training\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Find Latest Model Checkpoint\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m checkpoints = \u001b[38;5;28msorted\u001b[39m(\u001b[43mglob\u001b[49m\u001b[43m.\u001b[49m\u001b[43mglob\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33mvae_epoch*.pth\u001b[39m\u001b[33m\"\u001b[39m), key=os.path.getmtime, reverse=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m checkpoints:\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33m‚ùå No VAE checkpoint found in this folder. Train the model first!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'function' object has no attribute 'glob'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from auto_encoder_classes import VAE\n",
    "from auto_encoder_functions import vae_loss\n",
    "\n",
    "# --------------------------\n",
    "# Define Dataset Here\n",
    "# --------------------------\n",
    "class MRISliceDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset for MRI slices (no masks).\n",
    "    Loads all .png files in the given directory.\n",
    "    \"\"\"\n",
    "    def __init__(self, image_dir):\n",
    "        self.image_paths = sorted(glob(os.path.join(image_dir, \"*.png\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.image_paths[idx]).convert(\"L\")\n",
    "        img = np.array(img, dtype=np.float32) / 255.0\n",
    "        img = torch.tensor(img).unsqueeze(0)  # shape (1,H,W)\n",
    "        return img\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "TEST_DIR = \"../4.2/keras_png_slices_test\"\n",
    "BATCH_SIZE = 16\n",
    "LATENT_DIM = 32  # must match training\n",
    "\n",
    "# --------------------------\n",
    "# Find Latest Model Checkpoint\n",
    "# --------------------------\n",
    "checkpoints = sorted(glob.glob(\"vae_epoch*.pth\"), key=os.path.getmtime, reverse=True)\n",
    "if not checkpoints:\n",
    "    raise FileNotFoundError(\"‚ùå No VAE checkpoint found in this folder. Train the model first!\")\n",
    "MODEL_PATH = checkpoints[0]\n",
    "print(f\"üîé Loading latest checkpoint: {MODEL_PATH}\")\n",
    "\n",
    "# --------------------------\n",
    "# Load Dataset + Model\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dataset = MRISliceDataset(TEST_DIR)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "vae = VAE(latent_dim=LATENT_DIM, input_shape=(1, 256, 256)).to(device)\n",
    "vae.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "vae.eval()\n",
    "\n",
    "# --------------------------\n",
    "# 1. Compute Average Test Loss\n",
    "# --------------------------\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for imgs in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        recon, mu, logvar = vae(imgs)\n",
    "        loss = vae_loss(recon, imgs, mu, logvar)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Visualize Reconstructions\n",
    "# --------------------------\n",
    "imgs = next(iter(test_loader)).to(device)\n",
    "with torch.no_grad():\n",
    "    recon, mu, logvar = vae(imgs)\n",
    "\n",
    "n_show = min(8, imgs.size(0))\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(n_show):\n",
    "    # Input\n",
    "    plt.subplot(2, n_show, i + 1)\n",
    "    plt.imshow(imgs[i, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"Input\", fontsize=12)\n",
    "\n",
    "    # Reconstruction\n",
    "    plt.subplot(2, n_show, n_show + i + 1)\n",
    "    plt.imshow(recon[i, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"Recon\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"VAE Reconstructions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 3. Latent Space Visualization\n",
    "# --------------------------\n",
    "if LATENT_DIM > 2:\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            mu, _ = vae.encoder(imgs)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "\n",
    "    z_2d = PCA(n_components=2).fit_transform(latents)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(z_2d[:, 0], z_2d[:, 1], s=8, alpha=0.7)\n",
    "    plt.title(\"Latent Space (PCA Projection)\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "else:\n",
    "    all_mu = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            mu, _ = vae.encoder(imgs)\n",
    "            all_mu.append(mu.cpu().numpy())\n",
    "    all_mu = np.concatenate(all_mu, axis=0)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(all_mu[:, 0], all_mu[:, 1], s=8, alpha=0.7)\n",
    "    plt.title(\"Latent Space\")\n",
    "    plt.xlabel(\"z1\")\n",
    "    plt.ylabel(\"z2\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e22d0bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jamed\\AppData\\Local\\Temp\\ipykernel_4992\\2929014119.py:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  vae.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'best_vae.pth'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     26\u001b[39m vae = VAE(latent_dim=LATENT_DIM).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m vae.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     28\u001b[39m vae.eval()\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;66;03m# 1. Compute Average Test Loss\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# --------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jamed\\miniconda3\\envs\\ml_env_working_maybe\\Lib\\site-packages\\torch\\serialization.py:1319\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args.keys():\n\u001b[32m   1317\u001b[39m     pickle_load_args[\u001b[33m\"\u001b[39m\u001b[33mencoding\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1319\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[32m   1320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[32m   1321\u001b[39m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[32m   1322\u001b[39m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[32m   1323\u001b[39m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[32m   1324\u001b[39m         orig_position = opened_file.tell()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jamed\\miniconda3\\envs\\ml_env_working_maybe\\Lib\\site-packages\\torch\\serialization.py:659\u001b[39m, in \u001b[36m_open_file_like\u001b[39m\u001b[34m(name_or_buffer, mode)\u001b[39m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[32m    658\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    661\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jamed\\miniconda3\\envs\\ml_env_working_maybe\\Lib\\site-packages\\torch\\serialization.py:640\u001b[39m, in \u001b[36m_open_file.__init__\u001b[39m\u001b[34m(self, name, mode)\u001b[39m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[32m--> \u001b[39m\u001b[32m640\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'best_vae.pth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from auto_encoder_classes import VAE, MRISliceDataset\n",
    "from auto_encoder_functions import vae_loss\n",
    "\n",
    "# --------------------------\n",
    "# Config\n",
    "# --------------------------\n",
    "MODEL_PATH = \"best_vae.pth\"\n",
    "TEST_DIR = \"../4.2/keras_png_slices_test\"  # points to data in part_4_scirpts/4.2\n",
    "BATCH_SIZE = 16\n",
    "LATENT_DIM = 32  # must match what you trained with\n",
    "\n",
    "# --------------------------\n",
    "# Load Dataset + Model\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "test_dataset = MRISliceDataset(TEST_DIR)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "vae = VAE(latent_dim=LATENT_DIM).to(device)\n",
    "vae.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "vae.eval()\n",
    "\n",
    "# --------------------------\n",
    "# 1. Compute Average Test Loss\n",
    "# --------------------------\n",
    "total_loss = 0.0\n",
    "with torch.no_grad():\n",
    "    for imgs in test_loader:\n",
    "        imgs = imgs.to(device)\n",
    "        recon, mu, logvar = vae(imgs)\n",
    "        loss = vae_loss(recon, imgs, mu, logvar)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "avg_loss = total_loss / len(test_loader)\n",
    "print(f\"Average Test Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Visualize Reconstructions\n",
    "# --------------------------\n",
    "imgs = next(iter(test_loader)).to(device)\n",
    "with torch.no_grad():\n",
    "    recon, mu, logvar = vae(imgs)\n",
    "\n",
    "n_show = min(8, imgs.size(0))\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(n_show):\n",
    "    # Input\n",
    "    plt.subplot(2, n_show, i + 1)\n",
    "    plt.imshow(imgs[i, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"Input\", fontsize=12)\n",
    "    # Reconstruction\n",
    "    plt.subplot(2, n_show, n_show + i + 1)\n",
    "    plt.imshow(recon[i, 0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    if i == 0:\n",
    "        plt.ylabel(\"Recon\", fontsize=12)\n",
    "\n",
    "plt.suptitle(\"VAE Reconstructions\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --------------------------\n",
    "# 3. Latent Space Visualization\n",
    "# --------------------------\n",
    "if LATENT_DIM > 2:\n",
    "    latents = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            mu, _ = vae.encoder(imgs)\n",
    "            latents.append(mu.cpu().numpy())\n",
    "    latents = np.concatenate(latents, axis=0)\n",
    "\n",
    "    z_2d = PCA(n_components=2).fit_transform(latents)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(z_2d[:, 0], z_2d[:, 1], s=8, alpha=0.7)\n",
    "    plt.title(\"Latent Space (PCA Projection)\")\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n",
    "else:\n",
    "    # Direct plot if latent_dim == 2\n",
    "    all_mu = []\n",
    "    with torch.no_grad():\n",
    "        for imgs in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            mu, _ = vae.encoder(imgs)\n",
    "            all_mu.append(mu.cpu().numpy())\n",
    "    all_mu = np.concatenate(all_mu, axis=0)\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.scatter(all_mu[:, 0], all_mu[:, 1], s=8, alpha=0.7)\n",
    "    plt.title(\"Latent Space\")\n",
    "    plt.xlabel(\"z1\")\n",
    "    plt.ylabel(\"z2\")\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env_working_maybe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
